From a0305b4a007717f7d297c3c2a61d01f688f29847 Mon Sep 17 00:00:00 2001
From: wuchangsheng <wuchangsheng2@huawei.com>
Date: Tue, 30 Mar 2021 19:44:26 +0800
Subject: [PATCH] dpdk-support-gazelle-10-eal-memory-inter-config

---
 lib/librte_eal/linux/eal/eal_memory.c | 72 +++++++++++++++++++++++----
 1 file changed, 61 insertions(+), 11 deletions(-)

diff --git a/lib/librte_eal/linux/eal/eal_memory.c b/lib/librte_eal/linux/eal/eal_memory.c
index 43e4ffc..db70ac8 100644
--- a/lib/librte_eal/linux/eal/eal_memory.c
+++ b/lib/librte_eal/linux/eal/eal_memory.c
@@ -1055,10 +1055,10 @@ remap_needed_hugepages(struct hugepage_file *hugepages, int n_pages)
 		 * address to lower address. Here, physical addresses are in
 		 * descending order.
 		 */
-		else if ((prev->physaddr - cur->physaddr) != cur->size)
+		else if (!internal_config.map_perfect && (prev->physaddr - cur->physaddr) != cur->size)
 			new_memseg = 1;
 #else
-		else if ((cur->physaddr - prev->physaddr) != cur->size)
+		else if (!internal_config.map_perfect && (cur->physaddr - prev->physaddr) != cur->size)
 			new_memseg = 1;
 #endif
 
@@ -1457,6 +1457,24 @@ eal_legacy_hugepage_init(void)
 		/* meanwhile, also initialize used_hp hugepage sizes in used_hp */
 		used_hp[i].hugepage_sz = internal_config.hugepage_info[i].hugepage_sz;
 
+		if (internal_config.map_perfect) {
+			int sys_num_pages = 0;
+			int need_num_pages = 0;
+			struct rte_memseg_list *msl;
+		
+			for (j = 0; j < RTE_MAX_NUMA_NODES; j++) {
+				sys_num_pages += internal_config.hugepage_info[i].num_pages[j];
+			}
+		
+			for (j = 0; j < RTE_MAX_MEMSEG_LISTS; j++) {
+				msl = &mcfg->memsegs[j];
+				if (internal_config.hugepage_info[i].hugepage_sz == msl->page_sz)
+					need_num_pages += msl->memseg_arr.len;
+			}
+		
+			internal_config.hugepage_info[i].num_pages[0] = RTE_MIN(sys_num_pages, need_num_pages);
+		}
+
 		nr_hugepages += internal_config.hugepage_info[i].num_pages[0];
 	}
 
@@ -1537,8 +1555,13 @@ eal_legacy_hugepage_init(void)
 			goto fail;
 		}
 
-		qsort(&tmp_hp[hp_offset], hpi->num_pages[0],
-		      sizeof(struct hugepage_file), cmp_physaddr);
+		/* continuous physical memory does not bring performance improvements,
+		 * so no sorting is performed for quick startup.
+		 */
+		if (!internal_config.map_perfect) {
+			qsort(&tmp_hp[hp_offset], hpi->num_pages[0],
+					  sizeof(struct hugepage_file), cmp_physaddr);
+		}
 
 		/* we have processed a num of hugepages of this size, so inc offset */
 		hp_offset += hpi->num_pages[0];
@@ -2228,11 +2251,20 @@ memseg_primary_init(void)
 	uint64_t max_mem, max_mem_per_type;
 	unsigned int max_seglists_per_type;
 	unsigned int n_memtypes, cur_type;
+	struct hugepage_info used_hp[MAX_HUGEPAGE_SIZES];
 
 	/* no-huge does not need this at all */
 	if (internal_config.no_hugetlbfs)
 		return 0;
 
+	if (internal_config.map_perfect) {
+		memset(used_hp, 0, sizeof(used_hp));
+		ret = eal_sec_set_num_pages(&internal_config, used_hp);
+		if (ret == -1) {
+			RTE_LOG(ERR, EAL, "Cannot get num pages\n");
+		}
+	}
+
 	/*
 	 * figuring out amount of memory we're going to have is a long and very
 	 * involved process. the basic element we're operating with is a memory
@@ -2329,6 +2361,7 @@ memseg_primary_init(void)
 		struct memtype *type = &memtypes[cur_type];
 		uint64_t max_mem_per_list, pagesz;
 		int socket_id;
+		unsigned int need_n_segs, cur_n_segs;
 
 		pagesz = type->page_sz;
 		socket_id = type->socket_id;
@@ -2372,8 +2405,17 @@ memseg_primary_init(void)
 				"n_segs:%i socket_id:%i hugepage_sz:%" PRIu64 "\n",
 			n_seglists, n_segs, socket_id, pagesz);
 
+		if (internal_config.map_perfect)
+			need_n_segs = eal_sec_get_num_pages(used_hp, pagesz, socket_id);
+		else
+			need_n_segs = n_segs;
+
 		/* create all segment lists */
-		for (cur_seglist = 0; cur_seglist < n_seglists; cur_seglist++) {
+		for (cur_seglist = 0; cur_seglist < n_seglists && need_n_segs > 0; cur_seglist++) {
+			cur_n_segs = RTE_MIN(need_n_segs, n_segs);
+			if (internal_config.map_perfect)
+				need_n_segs -= cur_n_segs;
+
 			if (msl_idx >= RTE_MAX_MEMSEG_LISTS) {
 				RTE_LOG(ERR, EAL,
 					"No more space in memseg lists, please increase %s\n",
@@ -2400,9 +2442,10 @@ memseg_primary_init(void)
 }
 
 static int
-memseg_secondary_init(void)
+memseg_secondary_init(struct rte_config *rte_cfg,
+		const int switch_pri_and_sec, const int sec_idx)
 {
-	struct rte_mem_config *mcfg = rte_eal_get_configuration()->mem_config;
+	struct rte_mem_config *mcfg = rte_cfg->mem_config;
 	int msl_idx = 0;
 	struct rte_memseg_list *msl;
 
@@ -2414,7 +2457,7 @@ memseg_secondary_init(void)
 		if (msl->memseg_arr.len == 0)
 			continue;
 
-		if (rte_fbarray_attach(&msl->memseg_arr)) {
+		if (rte_sec_fbarray_attach(&msl->memseg_arr, switch_pri_and_sec, sec_idx)) {
 			RTE_LOG(ERR, EAL, "Cannot attach to primary process memseg lists\n");
 			return -1;
 		}
@@ -2430,11 +2473,18 @@ memseg_secondary_init(void)
 }
 
 int
-rte_eal_memseg_init(void)
+rte_eal_memseg_init(const int switch_pri_and_sec, const int sec_idx)
 {
 	/* increase rlimit to maximum */
 	struct rlimit lim;
 
+	struct rte_config *rte_cfg = NULL;
+	if (!switch_pri_and_sec) {
+		rte_cfg = rte_eal_get_configuration();
+	} else {
+		rte_cfg = rte_eal_sec_get_configuration(sec_idx);
+	}
+
 	if (getrlimit(RLIMIT_NOFILE, &lim) == 0) {
 		/* set limit to maximum */
 		lim.rlim_cur = lim.rlim_max;
@@ -2458,11 +2508,11 @@ rte_eal_memseg_init(void)
 	}
 #endif
 
-	return rte_eal_process_type() == RTE_PROC_PRIMARY ?
+	return rte_cfg->process_type == RTE_PROC_PRIMARY ?
 #ifndef RTE_ARCH_64
 			memseg_primary_init_32() :
 #else
 			memseg_primary_init() :
 #endif
-			memseg_secondary_init();
+			memseg_secondary_init(rte_cfg, switch_pri_and_sec, sec_idx);
 }
-- 
2.23.0

